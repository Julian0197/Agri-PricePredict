{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "# 导入模型\n",
    "import xgboost as xgb\n",
    "# 模型调参的工具\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Error metrics 评价指标\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库信息\n",
    "mysql_setting = {\n",
    "    'host': '47.100.201.211',\n",
    "    'port': 3306,\n",
    "    'user': 'root',\n",
    "    'passwd': 'iyGfLR64Ne4Ddhk7',\n",
    "    # 数据库名称\n",
    "    'db': 'data',\n",
    "    'charset': 'utf8'\n",
    "}\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{passwd}@{host}:{port}/{db}\".format(**mysql_setting), max_overflow=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑动窗口特征\n",
    "def mov_df(df, num):\n",
    "    df['mov_sales_avg'] = df['dayPrice'].transform(lambda x: x.rolling(num).mean()) \n",
    "    df['mov_sales_max'] = df['dayPrice'].transform(lambda x: x.rolling(num).max())\n",
    "    df['mov_sales_min'] = df['dayPrice'].transform(lambda x: x.rolling(num).min())\n",
    "    return df\n",
    "# 滑动窗口波动率特征\n",
    "def mov_rate(df):\n",
    "    df['滑窗价格波动'] = df['mov_sales_avg'].diff(periods=1)\n",
    "    l1 = df['mov_sales_avg'].tolist()\n",
    "    l2 = df['滑窗价格波动'].tolist()\n",
    "    def rate(list1, list2):\n",
    "        l = ['']\n",
    "        i = 1\n",
    "        while i <= len(list2)-1:\n",
    "            r = list2[i]/list1[i-1]\n",
    "            l.append(r)\n",
    "            i = i + 1\n",
    "        return l\n",
    "    df['滑窗价格波动率'] = rate(l1, l2)\n",
    "    df['滑窗价格波动率'] = pd.to_numeric(df['滑窗价格波动率'])\n",
    "    return df\n",
    "# 分割训练集预测集\n",
    "def train(df):\n",
    "    x_train = df.iloc[1:,2:]\n",
    "    y_train = df.iloc[1:,1]\n",
    "    x_train = x_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    x_predict = df.iloc[-4:,2:]\n",
    "    x_predict = x_predict.reset_index(drop=True)\n",
    "    return [x_train, y_train, x_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充缺失价格，去重重复天数价格\n",
    "def fillPrice(df):\n",
    "    # datetime.strptime() 把datetime转化为str格式\n",
    "    df['updateTime'] = df['updateTime'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    date_start = df['updateTime'].min()\n",
    "    date_end = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    #根据最小日期和最大日期，计算日期间隔，由于date_start和date_end为string类型，因此需要先更改为日期类型\n",
    "    delta = datetime.datetime.strptime(date_end, \"%Y-%m-%d\")-datetime.datetime.strptime(str(date_start), \"%Y-%m-%d\")\n",
    "    for i in range(1,delta.days+1):\n",
    "        date = (datetime.datetime.strptime(date_end, \"%Y-%m-%d\")-datetime.timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "        #如果数据缺失，则补数\n",
    "        if(date not in df['updateTime'].values):\n",
    "            new_date = pd.DataFrame({'updateTime':date,'dayPrice':np.nan}, index=[0])\n",
    "            df = df.append(new_date)\n",
    "    df.sort_values(by=\"updateTime\", inplace=True, ascending=True) \n",
    "    df['dayPrice'] = df['dayPrice'].fillna(df['dayPrice'].interpolate().round(2))\n",
    "    df['updateTime'] = pd.to_datetime(df['updateTime'])\n",
    "    df['updateTime'] = df['updateTime'].dt.strftime(\"%Y-%m\")\n",
    "    df = df.groupby('updateTime').mean('dayPrice').round(2)\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 长期预测线上商品\n",
    "def predict_online(url, product, w):\n",
    "    if ('jd' in url):\n",
    "        sql_cmd = \"SELECT updateTime, dayPrice FROM data.jingdong_day_price_nineCounty where commodityUrl = %(url)s\"\n",
    "    else:\n",
    "        sql_cmd = \"SELECT updateTime, dayPrice FROM data.tianmao_day_price_nineCounty where commodityId = %(url)s\"\n",
    "    df = pd.read_sql(sql_cmd, engine, params={'url': url})\n",
    "    df.drop(index=df[df['dayPrice']<=0].index,inplace=True)\n",
    "    df = fillPrice(df)\n",
    "    df['dayPrice'] = df['dayPrice'].apply(lambda x: (x / w))\n",
    "    mov_df(df, 1)\n",
    "    mov_rate(df)\n",
    "    x_train = train(df)[0]\n",
    "    y_train = train(df)[1]\n",
    "    x_predict = train(df)[2]\n",
    "    rf = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n",
    "    boruta = BorutaPy(\n",
    "       estimator = rf, \n",
    "       n_estimators = 'auto',\n",
    "       max_iter = 100 # number of trials to perform\n",
    "    )\n",
    "    # 模型训练\n",
    "    boruta.fit(np.array(x_train), np.array(y_train))\n",
    "    # 输出结果\n",
    "    green_area = x_train.columns[boruta.support_].to_list()\n",
    "    blue_area  = x_train.columns[boruta.support_weak_].to_list()\n",
    "    # 选择最优参数\n",
    "    # setup regressor\n",
    "    xgb_model = xgb.XGBRegressor() \n",
    "    # performance a grid search\n",
    "    tweaked_model = GridSearchCV(\n",
    "        xgb_model,   \n",
    "        {\n",
    "            'max_depth':[1,2,5,10,20],\n",
    "            'n_estimators':[20,30,50,70,100],\n",
    "            'learning_rate':[0.1,0.2,0.3,0.4,0.5]\n",
    "        },   \n",
    "        cv = 3,   \n",
    "        verbose = 1,\n",
    "        n_jobs = -1,  \n",
    "        scoring = 'neg_median_absolute_error')\n",
    "    tweaked_model.fit(x_train, y_train)\n",
    "    model1 = xgb.XGBRegressor(learning_rate = tweaked_model.best_params_['learning_rate'], \n",
    "                              max_depth = tweaked_model.best_params_['max_depth'], \n",
    "                              n_estimators = tweaked_model.best_params_['n_estimators'])\n",
    "    model1.fit(x_train,y_train)\n",
    "    Y_predict = model1.predict(x_predict).round(2)\n",
    "\n",
    "    pre_date = pd.period_range((datetime.datetime.now() + relativedelta(months=1)), periods=4, freq='M')\n",
    "    predict = pd.DataFrame(list(zip(pre_date,Y_predict)), columns = ['predict_date','predict_price'])\n",
    "    predict['product'] = product\n",
    "    predict['date'] = datetime.datetime.now().strftime('%Y-%m')\n",
    "    order = ['product','date','predict_date', 'predict_price']\n",
    "    predict = predict[order]\n",
    "    predict['predict_price'] = predict['predict_price'].round(2)\n",
    "    predict['predict_date'] = predict['predict_date'].apply(lambda x: str(x))\n",
    "    predict['predict_date'] = pd.to_datetime(predict['predict_date'])\n",
    "    predict['predict_date'] = predict['predict_date'].apply(lambda x: x.strftime('%Y-%m'))\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "aa = predict_online('https://item.jd.com/10033573567269.html', '巢湖稻虾米', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    4.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.5s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.5s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.5s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = engine.execute('TRUNCATE TABLE data.price_predict_long')\n",
    "data = pd.read_csv('allProduct.csv', encoding='utf-8')\n",
    "for row in data.iterrows():\n",
    "    try:\n",
    "        table = predict_online(row[1]['url'], row[1]['product'], row[1]['weight'])\n",
    "        table.to_sql('price_predict_long', con= engine, if_exists= 'append', index=False)\n",
    "    except:\n",
    "        print(row[1]['product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
