{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "# 导入模型\n",
    "import xgboost as xgb\n",
    "\n",
    "#模型调参的工具\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#Error metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库信息\n",
    "mysql_setting = {\n",
    "    'host': '47.100.201.211',\n",
    "    'port': 3306,\n",
    "    'user': 'root',\n",
    "    'passwd': 'iyGfLR64Ne4Ddhk7',\n",
    "    # 数据库名称\n",
    "    'db': 'data',\n",
    "    'charset': 'utf8'\n",
    "}\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{passwd}@{host}:{port}/{db}\".format(**mysql_setting), max_overflow=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mov_df(df, num):\n",
    "    df['mov_ori_avg'] = df['orign_price'].transform(lambda x: x.rolling(num).mean())\n",
    "    df['mov_ori_max'] = df['orign_price'].transform(lambda x: x.rolling(num).max())\n",
    "    df['mov_ori_min'] = df['orign_price'].transform(lambda x: x.rolling(num).min())\n",
    "    df['mov_ori_var'] = df['orign_price'].transform(lambda x: x.rolling(num).var())\n",
    "    return df\n",
    "def mov_rate(df):\n",
    "    df['滑窗产地价格波动'] = df['mov_ori_avg'].diff(periods=1)\n",
    "    l1 = df['mov_ori_avg'].tolist()\n",
    "    l2 = df['滑窗产地价格波动'].tolist()\n",
    "    def rate(list1, list2):\n",
    "        l = ['','','','','']\n",
    "        i = 5\n",
    "        while i <= len(list2)-1:\n",
    "            r = list2[i]/list1[i-1]\n",
    "            l.append(r)\n",
    "            i = i + 1\n",
    "        return l\n",
    "    df['滑窗产地价格波动率'] = rate(l1, l2)\n",
    "    df['滑窗产地价格波动率'] = pd.to_numeric(df['滑窗产地价格波动率'])\n",
    "    return df\n",
    "def train(df):\n",
    "    x_train = df.iloc[5:-15,2:]\n",
    "    y_train = df.iloc[5:-15,1]\n",
    "    x_train = x_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    x_predict = df.iloc[-15:,2:]\n",
    "    x_predict = x_predict.reset_index(drop=True)\n",
    "    return [x_train, y_train, x_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_offline_short(df):\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df.rename(columns={'时间':'date','当日价格':'orign_price'},inplace=True)\n",
    "    mov_df(df, 5)\n",
    "    mov_rate(df)\n",
    "    x_train = train(df)[0]\n",
    "    y_train = train(df)[1]\n",
    "    x_predict = train(df)[2]\n",
    "    rf = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n",
    "    boruta = BorutaPy(\n",
    "       estimator = rf, \n",
    "       n_estimators = 'auto',\n",
    "       max_iter = 100 # number of trials to perform\n",
    "    )\n",
    "    # 模型训练\n",
    "    boruta.fit(np.array(x_train), np.array(y_train))\n",
    "    # 输出结果\n",
    "    green_area = x_train.columns[boruta.support_].to_list()\n",
    "    blue_area  = x_train.columns[boruta.support_weak_].to_list()\n",
    "    # 选择最优参数\n",
    "    #重新选择特征\n",
    "    f = []\n",
    "    f.extend(green_area)\n",
    "    f.extend(blue_area)\n",
    "    x_train = x_train[f]\n",
    "    x_predict = x_predict[f]\n",
    "    # setup regressor\n",
    "    xgb_model = xgb.XGBRegressor() \n",
    "    # performance a grid search\n",
    "    tweaked_model = GridSearchCV(\n",
    "        xgb_model,   \n",
    "        {\n",
    "            'max_depth':[1,2,5,10,20],\n",
    "            'n_estimators':[20,30,50,70,100],\n",
    "            'learning_rate':[0.1,0.2,0.3,0.4,0.5]\n",
    "        },   \n",
    "        cv = 3,   \n",
    "        verbose = 1,\n",
    "        n_jobs = -1,  \n",
    "        scoring = 'neg_median_absolute_error')\n",
    "    tweaked_model.fit(x_train, y_train)\n",
    "    model1 = xgb.XGBRegressor(learning_rate = tweaked_model.best_params_['learning_rate'], \n",
    "                              max_depth = tweaked_model.best_params_['max_depth'], \n",
    "                              n_estimators = tweaked_model.best_params_['n_estimators'])\n",
    "    model1.fit(x_train,y_train)\n",
    "    Y_predict = model1.predict(x_predict).round(2)\n",
    "\n",
    "    start_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    end_date = (datetime.datetime.now() + timedelta(days=16)).strftime('%Y-%m-%d')\n",
    "    pre_date = pd.date_range(start = start_date, end = end_date)\n",
    "    predict = pd.DataFrame(list(zip(pre_date,Y_predict)), columns = ['date','price'])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中垾番茄\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.fanqie where 市场 like '%合肥%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "fanqie = dt.loc[dt['市场']=='合肥周谷堆农产品批发市场股份有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "# 万年贡米，奉新大米，井岗红米，丰城大米，巢湖大米，仙桃香米\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.canmi\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "wanniangongmi = dt.loc[dt['市场']=='萍乡市安源春蕾农副产品发展有限公司'].groupby('时间').mean('当日价格')\n",
    "fengxindami = dt.loc[dt['市场']=='新余市同盛实业集团有限责任公司'].groupby('时间').mean('当日价格')\n",
    "jingganghongmi = dt.loc[dt['市场']=='南昌深圳农产品中心批发市场有限公司'].groupby('时间').mean('当日价格')\n",
    "fengchengdami = dt.loc[dt['市场']=='江西崇仁江贸批发部'].groupby('时间').mean('当日价格')\n",
    "chaohudami = dt.loc[dt['市场']=='合肥周谷堆农产品批发市场股份有限公司'].groupby('时间').mean('当日价格')\n",
    "xiantaoxiangmi = dt.loc[dt['市场']=='武汉白沙洲农副产品大市场有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "# 江西紫皮大蒜 \n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.dasuan where 地区 like '江西%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "jiangxidasuan = dt.groupby('时间').mean('当日价格')\n",
    "\n",
    "#沔城藕\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.lianou where 地区 like '湖北%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "mianchenou = dt.loc[dt['市场']=='襄阳竹叶山洪沟投资有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "#永新酱姜， 上高白肉姜\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.shengjiang where 地区 like '江西%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "yongxinjiangjiang = dt.loc[dt['市场']=='乐平蔬菜批发大市场'].groupby('时间').mean('当日价格')\n",
    "shanggaobairoujiang = dt.loc[dt['市场']=='萍乡市安源春蕾农副产品发展有限公司'].groupby('时间').mean('当日价格')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    9.6s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    4.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    4.5s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    4.7s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    5.0s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    4.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    4.5s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    3.8s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    4.0s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 292 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    7.6s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    4.8s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    5.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    2.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.3s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    5.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    5.8s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    5.8s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predict = predict_offline_short(fanqie)\n",
    "predict['product'] = '中垾番茄'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df1 = predict\n",
    "\n",
    "predict = predict_offline_short(chaohudami)\n",
    "predict['product'] = '巢湖大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df2 = predict\n",
    "\n",
    "predict = predict_offline_short(wanniangongmi)\n",
    "predict['product'] = '万年贡米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df3 = predict\n",
    "\n",
    "predict = predict_offline_short(fengxindami)\n",
    "predict['product'] = '奉新大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df4 = predict\n",
    "\n",
    "predict = predict_offline_short(jingganghongmi)\n",
    "predict['product'] = '井岗红米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df5 = predict\n",
    "\n",
    "predict = predict_offline_short(fengchengdami)\n",
    "predict['product'] = '丰城大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df6 = predict\n",
    "\n",
    "predict = predict_offline_short(jiangxidasuan)\n",
    "predict['product'] = '江西紫皮大蒜'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df7 = predict\n",
    "\n",
    "predict = predict_offline_short(mianchenou)\n",
    "predict['product'] = '沔城藕'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df8 = predict\n",
    "\n",
    "predict = predict_offline_short(xiantaoxiangmi)\n",
    "predict['product'] = '仙桃香米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df9 = predict\n",
    "\n",
    "predict = predict_offline_short(yongxinjiangjiang)\n",
    "predict['product'] = '永新酱姜'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df10 = predict\n",
    "\n",
    "predict = predict_offline_short(shanggaobairoujiang)\n",
    "predict['product'] = '上高白肉姜'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df11 = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11]\n",
    "df = reduce(lambda x, y: x.append(y), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'TRUNCATE TABLE predict_offline_short'\n",
    "result = engine.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'predict_offline_short'\n",
    "df.to_sql(table_name, engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = center>商务部长期预测</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mov_df(df, num):\n",
    "    df['mov_ori_avg'] = df['orign_price'].transform(lambda x: x.rolling(num).mean())\n",
    "    df['mov_ori_max'] = df['orign_price'].transform(lambda x: x.rolling(num).max())\n",
    "    df['mov_ori_min'] = df['orign_price'].transform(lambda x: x.rolling(num).min())\n",
    "    df['mov_ori_var'] = df['orign_price'].transform(lambda x: x.rolling(num).var())\n",
    "    return df\n",
    "def mov_rate(df):\n",
    "    df['滑窗产地价格波动'] = df['mov_ori_avg'].diff(periods=1)\n",
    "    l1 = df['mov_ori_avg'].tolist()\n",
    "    l2 = df['滑窗产地价格波动'].tolist()\n",
    "    def rate(list1, list2):\n",
    "        l = ['','','','','']\n",
    "        i = 5\n",
    "        while i <= len(list2)-1:\n",
    "            r = list2[i]/list1[i-1]\n",
    "            l.append(r)\n",
    "            i = i + 1\n",
    "        return l\n",
    "    df['滑窗产地价格波动率'] = rate(l1, l2)\n",
    "    df['滑窗产地价格波动率'] = pd.to_numeric(df['滑窗产地价格波动率'])\n",
    "    return df\n",
    "def train(df):\n",
    "    x_train = df.iloc[5:-12,2:]\n",
    "    y_train = df.iloc[5:-12,1]\n",
    "    x_train = x_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    x_predict = df.iloc[-12:,2:]\n",
    "    x_predict = x_predict.reset_index(drop=True)\n",
    "    return [x_train, y_train, x_predict]\n",
    "def predict_offline_long(df):\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df.rename(columns={'时间':'date','当日价格':'orign_price'},inplace=True)\n",
    "    def deleteErrorDate(df):\n",
    "        df.drop(df[df['date']=='2019-02-29'].index, inplace= True)\n",
    "        df.drop(df[df['date']=='2019-02-30'].index, inplace= True)\n",
    "        df.drop(df[df['date']=='2019-02-31'].index, inplace= True)    \n",
    "        df.drop(df[df['date']=='2020-02-30'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2020-02-31'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2021-02-29'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2021-02-30'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2021-02-31'].index, inplace= True) \n",
    "        df.drop(df[df['date']=='2022-02-29'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2022-02-30'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2022-02-31'].index, inplace= True) \n",
    "        df.drop(df[df['date']=='2021-04-31'].index, inplace= True) \n",
    "        return df\n",
    "    df = deleteErrorDate(df)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['date'] = df['date'].dt.strftime(\"%Y-%m\")\n",
    "    df = df.groupby('date').mean('orign_price').round(2)\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    mov_df(df, 5)\n",
    "    mov_rate(df)\n",
    "    x_train = train(df)[0]\n",
    "    y_train = train(df)[1]\n",
    "    x_predict = train(df)[2]\n",
    "    rf = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n",
    "    boruta = BorutaPy(\n",
    "       estimator = rf, \n",
    "       n_estimators = 'auto',\n",
    "       max_iter = 100 # number of trials to perform\n",
    "    )\n",
    "    # 模型训练\n",
    "    boruta.fit(np.array(x_train), np.array(y_train))\n",
    "    # 输出结果\n",
    "    green_area = x_train.columns[boruta.support_].to_list()\n",
    "    blue_area  = x_train.columns[boruta.support_weak_].to_list()\n",
    "    # 选择最优参数\n",
    "    #重新选择特征\n",
    "    f = []\n",
    "    f.extend(green_area)\n",
    "    f.extend(blue_area)\n",
    "    x_train = x_train[f]\n",
    "    x_predict = x_predict[f]\n",
    "    # setup regressor\n",
    "    xgb_model = xgb.XGBRegressor() \n",
    "    # performance a grid search\n",
    "    tweaked_model = GridSearchCV(\n",
    "        xgb_model,   \n",
    "        {\n",
    "            'max_depth':[1,2,5,10,20],\n",
    "            'n_estimators':[20,30,50,70,100],\n",
    "            'learning_rate':[0.1,0.2,0.3,0.4,0.5]\n",
    "        },   \n",
    "        cv = 3,   \n",
    "        verbose = 1,\n",
    "        n_jobs = -1,  \n",
    "        scoring = 'neg_median_absolute_error')\n",
    "    tweaked_model.fit(x_train, y_train)\n",
    "    model1 = xgb.XGBRegressor(learning_rate = tweaked_model.best_params_['learning_rate'], \n",
    "                              max_depth = tweaked_model.best_params_['max_depth'], \n",
    "                              n_estimators = tweaked_model.best_params_['n_estimators'])\n",
    "    model1.fit(x_train,y_train)\n",
    "    Y_predict = model1.predict(x_predict).round(2)\n",
    "    pre_date = pd.period_range(datetime.datetime.now(), periods=12, freq='M')\n",
    "    predict = pd.DataFrame(list(zip(pre_date,Y_predict)), columns = ['date','price'])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中垾番茄\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.fanqie where 市场 like '%合肥%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "fanqie = dt.loc[dt['市场']=='合肥周谷堆农产品批发市场股份有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "# 万年贡米，奉新大米，井岗红米，丰城大米，巢湖大米，仙桃香米\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.canmi\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "wanniangongmi = dt.loc[dt['市场']=='萍乡市安源春蕾农副产品发展有限公司'].groupby('时间').mean('当日价格')\n",
    "fengxindami = dt.loc[dt['市场']=='新余市同盛实业集团有限责任公司'].groupby('时间').mean('当日价格')\n",
    "jingganghongmi = dt.loc[dt['市场']=='南昌深圳农产品中心批发市场有限公司'].groupby('时间').mean('当日价格')\n",
    "fengchengdami = dt.loc[dt['市场']=='江西崇仁江贸批发部'].groupby('时间').mean('当日价格')\n",
    "chaohudami = dt.loc[dt['市场']=='合肥周谷堆农产品批发市场股份有限公司'].groupby('时间').mean('当日价格')\n",
    "xiantaoxiangmi = dt.loc[dt['市场']=='武汉白沙洲农副产品大市场有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "# 江西紫皮大蒜 \n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.dasuan where 地区 like '江西%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "jiangxidasuan = dt.groupby('时间').mean('当日价格')\n",
    "\n",
    "#沔城藕\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.lianou where 地区 like '湖北%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "mianchenou = dt.loc[dt['市场']=='襄阳竹叶山洪沟投资有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "#永新酱姜， 上高白肉姜\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.shengjiang where 地区 like '江西%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "yongxinjiangjiang = dt.loc[dt['市场']=='乐平蔬菜批发大市场'].groupby('时间').mean('当日价格')\n",
    "shanggaobairoujiang = dt.loc[dt['市场']=='萍乡市安源春蕾农副产品发展有限公司'].groupby('时间').mean('当日价格')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.2s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.8s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.9s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.0s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    1.9s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.0s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.8s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    2.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[20:40:09] C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-e60e698de21e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# df10 = predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_offline_long\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmianchenou\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'沔城藕'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'product'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-6e0bfc437323>\u001b[0m in \u001b[0;36mpredict_offline_long\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         scoring = 'neg_median_absolute_error')\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mtweaked_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     model1 = xgb.XGBRegressor(learning_rate = tweaked_model.best_params_['learning_rate'], \n\u001b[0;32m     91\u001b[0m                               \u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweaked_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m    737\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1497\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [20:40:09] C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?"
     ]
    }
   ],
   "source": [
    "predict = predict_offline_long(fanqie)\n",
    "predict['product'] = '中垾番茄'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df1 = predict\n",
    "\n",
    "predict = predict_offline_long(chaohudami)\n",
    "predict['product'] = '巢湖大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df2 = predict\n",
    "\n",
    "predict = predict_offline_long(wanniangongmi)\n",
    "predict['product'] = '万年贡米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df3 = predict\n",
    "\n",
    "predict = predict_offline_long(fengxindami)\n",
    "predict['product'] = '奉新大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df4 = predict\n",
    "\n",
    "predict = predict_offline_long(jingganghongmi)\n",
    "predict['product'] = '井岗红米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df5 = predict\n",
    "\n",
    "predict = predict_offline_long(fengchengdami)\n",
    "predict['product'] = '丰城大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df6 = predict\n",
    "\n",
    "predict = predict_offline_long(jiangxidasuan)\n",
    "predict['product'] = '江西紫皮大蒜'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df7 = predict\n",
    "\n",
    "# predict = predict_offline_long(xiantaoxiangmi)\n",
    "# predict['product'] = '仙桃香米'\n",
    "# order = ['product','date','price']\n",
    "# predict = predict[order]\n",
    "# df9 = predict\n",
    "\n",
    "# predict = predict_offline_long(yongxinjiangjiang)\n",
    "# predict['product'] = '永新酱姜'\n",
    "# order = ['product','date','price']\n",
    "# predict = predict[order]\n",
    "# df10 = predict\n",
    "\n",
    "# predict = predict_offline_long(mianchenou)\n",
    "# predict['product'] = '沔城藕'\n",
    "# order = ['product','date','price']\n",
    "# predict = predict[order]\n",
    "# df8 = predict\n",
    "\n",
    "# predict = predict_offline_long(shanggaobairoujiang)\n",
    "# predict['product'] = '上高白肉姜'\n",
    "# order = ['product','date','price']\n",
    "# predict = predict[order]\n",
    "# df11 = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df1, df2, df3, df4, df5, df6, df7]\n",
    "df = reduce(lambda x, y: x.append(y), dfs)\n",
    "df['date'] = df['date'].apply(lambda x: str(x))\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'TRUNCATE TABLE predict_offline_long'\n",
    "result = engine.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'predict_offline_long'\n",
    "df.to_sql(table_name, engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
