{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "# 导入模型\n",
    "import xgboost as xgb\n",
    "\n",
    "#模型调参的工具\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#Error metrics评价指标\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库信息\n",
    "mysql_setting = {\n",
    "    'host': '47.100.201.211',\n",
    "    'port': 3306,\n",
    "    'user': 'root',\n",
    "    'passwd': 'iyGfLR64Ne4Ddhk7',\n",
    "    # 数据库名称\n",
    "    'db': 'data',\n",
    "    'charset': 'utf8'\n",
    "}\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{passwd}@{host}:{port}/{db}\".format(**mysql_setting), max_overflow=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mov_df(df, num):\n",
    "    df['mov_ori_avg'] = df['orign_price'].transform(lambda x: x.rolling(num).mean())\n",
    "    df['mov_ori_max'] = df['orign_price'].transform(lambda x: x.rolling(num).max())\n",
    "    df['mov_ori_min'] = df['orign_price'].transform(lambda x: x.rolling(num).min())\n",
    "    df['mov_ori_var'] = df['orign_price'].transform(lambda x: x.rolling(num).var())\n",
    "    return df\n",
    "def mov_rate(df):\n",
    "    df['滑窗产地价格波动'] = df['mov_ori_avg'].diff(periods=1)\n",
    "    l1 = df['mov_ori_avg'].tolist()\n",
    "    l2 = df['滑窗产地价格波动'].tolist()\n",
    "    def rate(list1, list2):\n",
    "        l = ['','','','','']\n",
    "        i = 5\n",
    "        while i <= len(list2)-1:\n",
    "            r = list2[i]/list1[i-1]\n",
    "            l.append(r)\n",
    "            i = i + 1\n",
    "        return l\n",
    "    df['滑窗产地价格波动率'] = rate(l1, l2)\n",
    "    df['滑窗产地价格波动率'] = pd.to_numeric(df['滑窗产地价格波动率'])\n",
    "    return df\n",
    "def train(df):\n",
    "    x_train = df.iloc[5:-15,2:]\n",
    "    y_train = df.iloc[5:-15,1]\n",
    "    x_train = x_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    x_predict = df.iloc[-15:,2:]\n",
    "    x_predict = x_predict.reset_index(drop=True)\n",
    "    return [x_train, y_train, x_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_offline_short(df):\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df.rename(columns={'时间':'date','当日价格':'orign_price'},inplace=True)\n",
    "    mov_df(df, 5)\n",
    "    mov_rate(df)\n",
    "    x_train = train(df)[0]\n",
    "    y_train = train(df)[1]\n",
    "    x_predict = train(df)[2]\n",
    "    rf = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n",
    "    boruta = BorutaPy(\n",
    "       estimator = rf, \n",
    "       n_estimators = 'auto',\n",
    "       max_iter = 100 # number of trials to perform\n",
    "    )\n",
    "    # 模型训练\n",
    "    boruta.fit(np.array(x_train), np.array(y_train))\n",
    "    # 输出结果\n",
    "    green_area = x_train.columns[boruta.support_].to_list()\n",
    "    blue_area  = x_train.columns[boruta.support_weak_].to_list()\n",
    "    # 选择最优参数\n",
    "    #重新选择特征\n",
    "    f = []\n",
    "    f.extend(green_area)\n",
    "    f.extend(blue_area)\n",
    "    x_train = x_train[f]\n",
    "    x_predict = x_predict[f]\n",
    "    # setup regressor\n",
    "    xgb_model = xgb.XGBRegressor() \n",
    "    # performance a grid search\n",
    "    tweaked_model = GridSearchCV(\n",
    "        xgb_model,   \n",
    "        {\n",
    "            'max_depth':[1,2,5,10,20],\n",
    "            'n_estimators':[20,30,50,70,100],\n",
    "            'learning_rate':[0.1,0.2,0.3,0.4,0.5]\n",
    "        },   \n",
    "        cv = 3,   \n",
    "        verbose = 1,\n",
    "        n_jobs = -1,  \n",
    "        scoring = 'neg_median_absolute_error')\n",
    "    tweaked_model.fit(x_train, y_train)\n",
    "    model1 = xgb.XGBRegressor(learning_rate = tweaked_model.best_params_['learning_rate'], \n",
    "                              max_depth = tweaked_model.best_params_['max_depth'], \n",
    "                              n_estimators = tweaked_model.best_params_['n_estimators'])\n",
    "    model1.fit(x_train,y_train)\n",
    "    Y_predict = model1.predict(x_predict).round(2)\n",
    "\n",
    "    start_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    end_date = (datetime.datetime.now() + timedelta(days=16)).strftime('%Y-%m-%d')\n",
    "    pre_date = pd.date_range(start = start_date, end = end_date)\n",
    "    predict = pd.DataFrame(list(zip(pre_date,Y_predict)), columns = ['date','price'])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中垾番茄\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.fanqie where 市场 like '%合肥%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "fanqie = dt.loc[dt['市场']=='合肥周谷堆农产品批发市场股份有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "# 万年贡米，奉新大米，井岗红米，丰城大米，巢湖大米，仙桃香米\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.canmi\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "wanniangongmi = dt.loc[dt['市场']=='萍乡市安源春蕾农副产品发展有限公司'].groupby('时间').mean('当日价格')\n",
    "fengxindami = dt.loc[dt['市场']=='新余市同盛实业集团有限责任公司'].groupby('时间').mean('当日价格')\n",
    "jingganghongmi = dt.loc[dt['市场']=='南昌深圳农产品中心批发市场有限公司'].groupby('时间').mean('当日价格')\n",
    "fengchengdami = dt.loc[dt['市场']=='江西崇仁江贸批发部'].groupby('时间').mean('当日价格')\n",
    "chaohudami = dt.loc[dt['市场']=='合肥周谷堆农产品批发市场股份有限公司'].groupby('时间').mean('当日价格')\n",
    "xiantaoxiangmi = dt.loc[dt['市场']=='武汉白沙洲农副产品大市场有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "# 江西紫皮大蒜 \n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.dasuan where 地区 like '江西%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "jiangxidasuan = dt.groupby('时间').mean('当日价格')\n",
    "\n",
    "#沔城藕\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.lianou where 地区 like '湖北%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "mianchenou = dt.loc[dt['市场']=='襄阳竹叶山洪沟投资有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "#永新酱姜， 上高白肉姜\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.shengjiang where 地区 like '江西%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "yongxinjiangjiang = dt.loc[dt['市场']=='乐平蔬菜批发大市场'].groupby('时间').mean('当日价格')\n",
    "shanggaobairoujiang = dt.loc[dt['市场']=='萍乡市安源春蕾农副产品发展有限公司'].groupby('时间').mean('当日价格')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:   16.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    6.6s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    7.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    8.2s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    7.5s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    8.5s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    7.0s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    7.7s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    4.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    4.6s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:   19.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    9.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    4.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    4.4s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    7.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 293 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    7.6s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predict = predict_offline_short(fanqie)\n",
    "predict['product'] = '中垾番茄'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df1 = predict\n",
    "\n",
    "predict = predict_offline_short(chaohudami)\n",
    "predict['product'] = '巢湖大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df2 = predict\n",
    "\n",
    "predict = predict_offline_short(wanniangongmi)\n",
    "predict['product'] = '万年贡米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df3 = predict\n",
    "\n",
    "predict = predict_offline_short(fengxindami)\n",
    "predict['product'] = '奉新大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df4 = predict\n",
    "\n",
    "predict = predict_offline_short(jingganghongmi)\n",
    "predict['product'] = '井岗红米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df5 = predict\n",
    "\n",
    "predict = predict_offline_short(fengchengdami)\n",
    "predict['product'] = '丰城大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df6 = predict\n",
    "\n",
    "predict = predict_offline_short(jiangxidasuan)\n",
    "predict['product'] = '江西紫皮大蒜'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df7 = predict\n",
    "\n",
    "predict = predict_offline_short(mianchenou)\n",
    "predict['product'] = '沔城藕'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df8 = predict\n",
    "\n",
    "predict = predict_offline_short(xiantaoxiangmi)\n",
    "predict['product'] = '仙桃香米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df9 = predict\n",
    "\n",
    "predict = predict_offline_short(yongxinjiangjiang)\n",
    "predict['product'] = '永新酱姜'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df10 = predict\n",
    "\n",
    "predict = predict_offline_short(shanggaobairoujiang)\n",
    "predict['product'] = '上高白肉姜'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df11 = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11]\n",
    "df = reduce(lambda x, y: x.append(y), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'TRUNCATE TABLE predict_offline_short'\n",
    "result = engine.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'predict_offline_short'\n",
    "df.to_sql(table_name, engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = center>商务部长期预测</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mov_df(df, num):\n",
    "    df['mov_ori_avg'] = df['orign_price'].transform(lambda x: x.rolling(num).mean())\n",
    "    df['mov_ori_max'] = df['orign_price'].transform(lambda x: x.rolling(num).max())\n",
    "    df['mov_ori_min'] = df['orign_price'].transform(lambda x: x.rolling(num).min())\n",
    "    df['mov_ori_var'] = df['orign_price'].transform(lambda x: x.rolling(num).var())\n",
    "    return df\n",
    "def mov_rate(df):\n",
    "    df['滑窗产地价格波动'] = df['mov_ori_avg'].diff(periods=1)\n",
    "    l1 = df['mov_ori_avg'].tolist()\n",
    "    l2 = df['滑窗产地价格波动'].tolist()\n",
    "    def rate(list1, list2):\n",
    "        l = ['','','','','']\n",
    "        i = 5\n",
    "        while i <= len(list2)-1:\n",
    "            r = list2[i]/list1[i-1]\n",
    "            l.append(r)\n",
    "            i = i + 1\n",
    "        return l\n",
    "    df['滑窗产地价格波动率'] = rate(l1, l2)\n",
    "    df['滑窗产地价格波动率'] = pd.to_numeric(df['滑窗产地价格波动率'])\n",
    "    return df\n",
    "def train(df):\n",
    "    x_train = df.iloc[5:-12,2:]\n",
    "    y_train = df.iloc[5:-12,1]\n",
    "    x_train = x_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    x_predict = df.iloc[-12:,2:]\n",
    "    x_predict = x_predict.reset_index(drop=True)\n",
    "    return [x_train, y_train, x_predict]\n",
    "def predict_offline_long(df):\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df.rename(columns={'时间':'date','当日价格':'orign_price'},inplace=True)\n",
    "    def deleteErrorDate(df):\n",
    "        df.drop(df[df['date']=='2019-02-29'].index, inplace= True)\n",
    "        df.drop(df[df['date']=='2019-02-30'].index, inplace= True)\n",
    "        df.drop(df[df['date']=='2019-02-31'].index, inplace= True)    \n",
    "        df.drop(df[df['date']=='2020-02-30'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2020-02-31'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2021-02-29'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2021-02-30'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2021-02-31'].index, inplace= True) \n",
    "        df.drop(df[df['date']=='2022-02-29'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2022-02-30'].index, inplace= True)  \n",
    "        df.drop(df[df['date']=='2022-02-31'].index, inplace= True) \n",
    "        df.drop(df[df['date']=='2021-04-31'].index, inplace= True) \n",
    "        return df\n",
    "    df = deleteErrorDate(df)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['date'] = df['date'].dt.strftime(\"%Y-%m\")\n",
    "    df = df.groupby('date').mean('orign_price').round(2)\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    mov_df(df, 5)\n",
    "    mov_rate(df)\n",
    "    x_train = train(df)[0]\n",
    "    y_train = train(df)[1]\n",
    "    x_predict = train(df)[2]\n",
    "    rf = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n",
    "    boruta = BorutaPy(\n",
    "       estimator = rf, \n",
    "       n_estimators = 'auto',\n",
    "       max_iter = 100 # number of trials to perform\n",
    "    )\n",
    "    # 模型训练\n",
    "    boruta.fit(np.array(x_train), np.array(y_train))\n",
    "    # 输出结果\n",
    "    green_area = x_train.columns[boruta.support_].to_list()\n",
    "    blue_area  = x_train.columns[boruta.support_weak_].to_list()\n",
    "    # 选择最优参数\n",
    "    #重新选择特征\n",
    "    f = []\n",
    "    f.extend(green_area)\n",
    "    f.extend(blue_area)\n",
    "    x_train = x_train[f]\n",
    "    x_predict = x_predict[f]\n",
    "    # setup regressor\n",
    "    xgb_model = xgb.XGBRegressor() \n",
    "    # performance a grid search\n",
    "    tweaked_model = GridSearchCV(\n",
    "        xgb_model,   \n",
    "        {\n",
    "            'max_depth':[1,2,5,10,20],\n",
    "            'n_estimators':[20,30,50,70,100],\n",
    "            'learning_rate':[0.1,0.2,0.3,0.4,0.5]\n",
    "        },   \n",
    "        cv = 3,   \n",
    "        verbose = 1,\n",
    "        n_jobs = -1,  \n",
    "        scoring = 'neg_median_absolute_error')\n",
    "    tweaked_model.fit(x_train, y_train)\n",
    "    model1 = xgb.XGBRegressor(learning_rate = tweaked_model.best_params_['learning_rate'], \n",
    "                              max_depth = tweaked_model.best_params_['max_depth'], \n",
    "                              n_estimators = tweaked_model.best_params_['n_estimators'])\n",
    "    model1.fit(x_train,y_train)\n",
    "    Y_predict = model1.predict(x_predict).round(2)\n",
    "    pre_date = pd.period_range(datetime.datetime.now(), periods=12, freq='M')\n",
    "    predict = pd.DataFrame(list(zip(pre_date,Y_predict)), columns = ['date','price'])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中垾番茄\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.fanqie where 市场 like '%合肥%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "fanqie = dt.loc[dt['市场']=='合肥周谷堆农产品批发市场股份有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "# 万年贡米，奉新大米，井岗红米，丰城大米，巢湖大米，仙桃香米\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.canmi\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "wanniangongmi = dt.loc[dt['市场']=='萍乡市安源春蕾农副产品发展有限公司'].groupby('时间').mean('当日价格')\n",
    "fengxindami = dt.loc[dt['市场']=='新余市同盛实业集团有限责任公司'].groupby('时间').mean('当日价格')\n",
    "jingganghongmi = dt.loc[dt['市场']=='南昌深圳农产品中心批发市场有限公司'].groupby('时间').mean('当日价格')\n",
    "fengchengdami = dt.loc[dt['市场']=='江西崇仁江贸批发部'].groupby('时间').mean('当日价格')\n",
    "chaohudami = dt.loc[dt['市场']=='合肥周谷堆农产品批发市场股份有限公司'].groupby('时间').mean('当日价格')\n",
    "xiantaoxiangmi = dt.loc[dt['市场']=='武汉白沙洲农副产品大市场有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "# 江西紫皮大蒜 \n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.dasuan where 地区 like '江西%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "jiangxidasuan = dt.groupby('时间').mean('当日价格')\n",
    "\n",
    "#沔城藕\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.lianou where 地区 like '湖北%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "mianchenou = dt.loc[dt['市场']=='襄阳竹叶山洪沟投资有限公司'].groupby('时间').mean('当日价格')\n",
    "\n",
    "#永新酱姜， 上高白肉姜\n",
    "sql_cmd = \"SELECT 时间,市场,当日价格 FROM data.shengjiang where 地区 like '江西%'\"\n",
    "dt = pd.read_sql(sql=sql_cmd, con=engine,parse_dates=0,coerce_float=2)\n",
    "yongxinjiangjiang = dt.loc[dt['市场']=='乐平蔬菜批发大市场'].groupby('时间').mean('当日价格')\n",
    "shanggaobairoujiang = dt.loc[dt['市场']=='萍乡市安源春蕾农副产品发展有限公司'].groupby('时间').mean('当日价格')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    2.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.2s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    2.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.0s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    2.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    2.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    1.9s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    2.2s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predict = predict_offline_long(fanqie)\n",
    "predict['product'] = '中垾番茄'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df1 = predict\n",
    "\n",
    "predict = predict_offline_long(chaohudami)\n",
    "predict['product'] = '巢湖大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df2 = predict\n",
    "\n",
    "predict = predict_offline_long(wanniangongmi)\n",
    "predict['product'] = '万年贡米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df3 = predict\n",
    "\n",
    "predict = predict_offline_long(fengxindami)\n",
    "predict['product'] = '奉新大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df4 = predict\n",
    "\n",
    "predict = predict_offline_long(jingganghongmi)\n",
    "predict['product'] = '井岗红米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df5 = predict\n",
    "\n",
    "predict = predict_offline_long(fengchengdami)\n",
    "predict['product'] = '丰城大米'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df6 = predict\n",
    "\n",
    "predict = predict_offline_long(jiangxidasuan)\n",
    "predict['product'] = '江西紫皮大蒜'\n",
    "order = ['product','date','price']\n",
    "predict = predict[order]\n",
    "df7 = predict\n",
    "\n",
    "# predict = predict_offline_long(xiantaoxiangmi)\n",
    "# predict['product'] = '仙桃香米'\n",
    "# order = ['product','date','price']\n",
    "# predict = predict[order]\n",
    "# df9 = predict\n",
    "\n",
    "# predict = predict_offline_long(yongxinjiangjiang)\n",
    "# predict['product'] = '永新酱姜'\n",
    "# order = ['product','date','price']\n",
    "# predict = predict[order]\n",
    "# df10 = predict\n",
    "\n",
    "# predict = predict_offline_long(mianchenou)\n",
    "# predict['product'] = '沔城藕'\n",
    "# order = ['product','date','price']\n",
    "# predict = predict[order]\n",
    "# df8 = predict\n",
    "\n",
    "# predict = predict_offline_long(shanggaobairoujiang)\n",
    "# predict['product'] = '上高白肉姜'\n",
    "# order = ['product','date','price']\n",
    "# predict = predict[order]\n",
    "# df11 = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df1, df2, df3, df4, df5, df6, df7]\n",
    "df = reduce(lambda x, y: x.append(y), dfs)\n",
    "df['date'] = df['date'].apply(lambda x: str(x))\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'TRUNCATE TABLE predict_offline_long'\n",
    "result = engine.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'predict_offline_long'\n",
    "df.to_sql(table_name, engine, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
